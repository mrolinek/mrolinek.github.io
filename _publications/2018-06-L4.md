---
title: "L4: Practical loss-based stepsize adaptation for deep learning"
collection: publications
permalink: /publication/2018-06-L4
excerpt: 'Is Adam really the best we can do? An simple enough update rule can dramatically outperform Adam on some datasets. The optimizer turned out not to be very robust but it had its moments such as actually driving the training loss on MNIST to 0.0 in 20 epochs.'
venue: 'NIPS 2018'
authors: 'Michal Rolinek, Georg Martius'
arxiv_link: 'https://arxiv.org/abs/1802.05074'
github_link: 'https://github.com/martius-lab/l4-optimizer'
---

**{{page.venue}}**

Authors: {{ page.authors }}

{{ page.excerpt }}


Links: [Arxiv]({{ page.arxiv_link }}) [Github]({{ page.github_link }})

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Tired of tuning parameters of SGD or Adam for <a href="https://twitter.com/hashtag/DeepLearning?src=hash&amp;ref_src=twsrc%5Etfw">#DeepLearning</a>? Our new optimizer (<a href="https://t.co/90hi80ghna">https://t.co/90hi80ghna</a>) works much better than the best constant learning rates. Try it out: <a href="https://twitter.com/hashtag/Tensorflow?src=hash&amp;ref_src=twsrc%5Etfw">#Tensorflow</a> code included, see <a href="https://t.co/k4YVzeqJrF">https://t.co/k4YVzeqJrF</a> <a href="https://t.co/qmBxsYWYgA">pic.twitter.com/qmBxsYWYgA</a></p>&mdash; Georg Martius (@GMartius) <a href="https://twitter.com/GMartius/status/965703255878664194?ref_src=twsrc%5Etfw">February 19, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

